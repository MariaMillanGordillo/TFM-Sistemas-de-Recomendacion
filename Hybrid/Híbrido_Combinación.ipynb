{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a17081f",
      "metadata": {
        "id": "6a17081f"
      },
      "source": [
        "# Hybrid System: Content Based + Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "126da538",
      "metadata": {
        "id": "126da538"
      },
      "source": [
        "Desarrollamos un modelo híbrido en el que combinamos los **embeddings de ítems obtenidos por CBF** (usando TF-IDF) con los **embeddings de ítems y usuarios generados por CF** (LightGCN). Esta representación conjunta se introduce en una red neuronal (MLP) que aprende a predecir la probabilidad de interacción, aprovechando tanto las relaciones estructurales del grafo como la información semántica de los ítems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fddf08bb",
      "metadata": {
        "id": "fddf08bb"
      },
      "source": [
        "## Importación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "JB6lCE_kVkWi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB6lCE_kVkWi",
        "outputId": "02d6b4c1-9461-467b-c35b-8e8f74e2785f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (781.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 torchaudio-2.3.0+cu121 torchvision-0.18.0+cu121 triton-2.3.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.4/950.4 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Reinstala PyTorch 2.3.0 con CUDA 12.1 (estable en Colab)\n",
        "!pip install torch==2.3.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Instala PyG y extensiones con soporte oficial\n",
        "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
        "  -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
        "!pip install -q torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "992ca00d",
      "metadata": {
        "id": "992ca00d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim, Tensor\n",
        "\n",
        "from torch_geometric.typing import Adj\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "from torch_geometric.utils import structured_negative_sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b50d92a3",
      "metadata": {
        "id": "b50d92a3"
      },
      "outputs": [],
      "source": [
        "# Dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc04fb4",
      "metadata": {
        "id": "9cc04fb4"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5afcb48d",
      "metadata": {
        "id": "5afcb48d"
      },
      "outputs": [],
      "source": [
        "interactions = pd.read_csv('/content/interactions_filtered.csv')\n",
        "books = pd.read_csv('/content/books_authors_genres.csv')\n",
        "tfidf_df = pd.read_parquet('/content/tf-idf.parquet', engine='pyarrow')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "134dd5f7",
      "metadata": {
        "id": "134dd5f7"
      },
      "source": [
        "Aplicamos un filtrado para eliminar ítems con pocas interacciones y usuarios poco activos, mejorando así la calidad del grafo de interacciones. Esto reduce la dispersión y mejora el aprendizaje del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bbc546c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbc546c7",
        "outputId": "7cd13eee-554a-4df4-d91d-4f3bb2b72fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interactions filtrado: (16803, 5)\n",
            "num_users=3310, num_items=947\n"
          ]
        }
      ],
      "source": [
        "item_inter_counts = interactions.groupby(\"book_id\").size()\n",
        "popular_books = item_inter_counts[item_inter_counts >= 10].index\n",
        "interactions = interactions[interactions[\"book_id\"].isin(popular_books)]\n",
        "\n",
        "user_inter_counts = interactions.groupby('user_id').size()\n",
        "active_users = user_inter_counts[user_inter_counts >= 4].index\n",
        "interactions = interactions[interactions['user_id'].isin(active_users)]\n",
        "\n",
        "print(f\"Interactions filtrado: {interactions.shape}\")\n",
        "\n",
        "num_users = interactions['user_id'].nunique()\n",
        "num_items = interactions['book_id'].nunique()\n",
        "print(f\"{num_users=}, {num_items=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4d3a40",
      "metadata": {
        "id": "7a4d3a40"
      },
      "source": [
        "Primero, mapeamos cada `book_id` a su correspondiente `work_id` y creamos una asignación de libros a índices internos. Después, filtramos aquellos libros que cuentan con una representación TF-IDF válida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dc8f90b6",
      "metadata": {
        "id": "dc8f90b6"
      },
      "outputs": [],
      "source": [
        "book_to_work = dict(zip(books['book_id'], books['work_id']))\n",
        "\n",
        "# Mapeo de libros a índices\n",
        "book_id_path = '/content/book_id_map.csv'\n",
        "book_mapping_df = pd.read_csv(book_id_path, index_col='book_id_csv')\n",
        "book_mapping = {index: i for i, index in enumerate(book_mapping_df.index.unique())}\n",
        "\n",
        "# Filtrado de libros válidos (con TF-IDF)\n",
        "valid_book_ids = []\n",
        "embedding_indices = []\n",
        "\n",
        "for book_id, idx in book_mapping.items():\n",
        "    work_id = book_to_work.get(book_id)\n",
        "    if work_id in tfidf_df.index:\n",
        "        valid_book_ids.append(book_id)\n",
        "        embedding_indices.append(idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bacb7ee2",
      "metadata": {
        "id": "bacb7ee2"
      },
      "source": [
        "Cargamos los embeddings aprendidos por LightGCN (CF) y los filtramos para quedarnos solo con los ítems válidos.\n",
        "\n",
        "Después, obtenemos las representaciones TF-IDF reducidas con PCA y normalizadas.\n",
        "\n",
        "Finalmente, concatenamos ambas representaciones para obtener embeddings híbridos que combinan información colaborativa y de contenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "F0L81pHwpJD6",
      "metadata": {
        "id": "F0L81pHwpJD6"
      },
      "outputs": [],
      "source": [
        "# Cargamos los embeddings de LightGCN\n",
        "checkpoint = torch.load('/content/lightgcn_CF_final_embeddings.pt', map_location=device)\n",
        "user_embeddings_gcn = checkpoint['users_emb_final_CF'].to(device)   # (num_users, emb_dim)\n",
        "item_embeddings_gcn = checkpoint['items_emb_final_CF'].to(device)   # (num_items, emb_dim)\n",
        "\n",
        "# Filtrar embeddings de ítems\n",
        "item_embeddings_gcn_filtered = item_embeddings_gcn[embedding_indices].to(device)\n",
        "\n",
        "# Filtrar TF-IDF\n",
        "work_ids = [book_to_work[book_id] for book_id in valid_book_ids]\n",
        "tfidf_vectors = tfidf_df.loc[work_ids].values\n",
        "\n",
        "# Reducción de dimensionalidad con PCA\n",
        "tfidf_dim = 128\n",
        "pca = PCA(n_components=tfidf_dim)\n",
        "tfidf_tensor_filtered = torch.tensor(pca.fit_transform(tfidf_vectors), dtype=torch.float32).to(device)\n",
        "\n",
        "# Normalizamos TF-IDF\n",
        "tfidf_tensor_filtered = F.normalize(tfidf_tensor_filtered)\n",
        "\n",
        "# Concatenamos embeddings de GCN y TF-IDF\n",
        "hybrid_item_embeddings = torch.cat([item_embeddings_gcn_filtered, tfidf_tensor_filtered], dim=1).to(device)  # (num_items, emb_dim + tfidf_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62cf354",
      "metadata": {
        "id": "b62cf354"
      },
      "source": [
        "Generamos un dataset balanceado de interacciones positivas y negativas. Para cada interacción real positiva, añadimos una negativa simulada seleccionando aleatoriamente un ítem que el usuario no haya valorado, con el objetivo de entrenar modelos supervisados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "WrVzEeJwpNnI",
      "metadata": {
        "id": "WrVzEeJwpNnI"
      },
      "outputs": [],
      "source": [
        "# Obtener todos los IDs únicos de usuarios y libros\n",
        "all_user_ids = interactions['user_id'].unique()\n",
        "all_book_ids = interactions['book_id'].unique()\n",
        "\n",
        "# Crear un diccionario: usuario → conjunto de libros con los que ha interactuado\n",
        "user_to_books = interactions.groupby('user_id')['book_id'].apply(set).to_dict()\n",
        "\n",
        "# Lista de interacciones positivas\n",
        "interactions_list = []\n",
        "for _, row in interactions.iterrows():\n",
        "    interactions_list.append((row['user_id'], row['book_id'], 1))\n",
        "\n",
        "# Número de ejemplos negativos que se van a generar\n",
        "num_negatives = len(interactions_list)\n",
        "negatives = set()\n",
        "\n",
        "# Generar interacciones negativas donde el usuario no haya interactuado con ese libro\n",
        "while len(negatives) < num_negatives:\n",
        "    user_id = random.choice(all_user_ids)\n",
        "    book_id = random.choice(all_book_ids)\n",
        "\n",
        "    # Solo agregar si no es una interacción positiva existente\n",
        "    if book_id not in user_to_books.get(user_id, set()):\n",
        "        negatives.add((user_id, book_id))\n",
        "\n",
        "# Convertir a lista de interacciones negativas\n",
        "for user_id, book_id in negatives:\n",
        "    interactions_list.append((user_id, book_id, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5adcc944",
      "metadata": {
        "id": "5adcc944"
      },
      "source": [
        "## Implementación del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0abec09",
      "metadata": {
        "id": "f0abec09"
      },
      "source": [
        "### Definición del modelo y del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a4e923",
      "metadata": {
        "id": "e5a4e923"
      },
      "source": [
        "Definimos el modelo híbrido `HybridRecommenderNN`, una red neuronal que combina embeddings obtenidos mediante **Content-Based Filtering (TF-IDF reducido con PCA)** y **Collaborative Filtering (LightGCN)**. La red aprende una representación conjunta a partir de ambas fuentes de información para predecir la probabilidad de interacción entre usuario e ítem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ozXGOpl1KHvp",
      "metadata": {
        "id": "ozXGOpl1KHvp"
      },
      "outputs": [],
      "source": [
        "class HybridRecommenderNN(nn.Module):\n",
        "    def __init__(self, gcn_dim, tfidf_dim, hidden_dims=[128, 64], dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        input_dim = gcn_dim + tfidf_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dims[0]) # Capa densa\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dims[0])  # Normalización\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1]) # Capa densa\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dims[1])\n",
        "\n",
        "        self.output = nn.Linear(hidden_dims[1], 1) # Salida para clasificación binaria\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, gcn_emb, tfidf_emb):\n",
        "        x = torch.cat([gcn_emb, tfidf_emb], dim=1)\n",
        "\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x.squeeze()  # (batch, 1) → (batch,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49dbb285",
      "metadata": {
        "id": "49dbb285"
      },
      "source": [
        "Definimos el dataset personalizado `HybridDataset`, que se encarga de preparar las muestras para el entrenamiento del modelo híbrido.\n",
        "\n",
        "Cada muestra combina la interacción usuario-ítem a través del producto de embeddings de LightGCN, junto con la representación TF-IDF del ítem, y su etiqueta binaria (positiva o negativa).\n",
        "\n",
        "Además, se incluye una función auxiliar `split_interactions` que divide las interacciones en subconjuntos de entrenamiento, validación y prueba de manera aleatoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fab40996",
      "metadata": {
        "id": "fab40996"
      },
      "outputs": [],
      "source": [
        "class HybridDataset(Dataset):\n",
        "    def __init__(self, interactions, user_embeddings_gcn, item_embeddings_gcn_filtered, tfidf_tensor_filtered, book_mapping, valid_book_ids):\n",
        "        self.samples = []\n",
        "        valid_books_set = set(valid_book_ids)\n",
        "        book_id_to_filtered_idx = {book_id: i for i, book_id in enumerate(valid_book_ids)}\n",
        "\n",
        "        # Construimos las muestras válidas a partir de las interacciones\n",
        "        for user_id, book_id, label in interactions:\n",
        "            if book_id in valid_books_set:\n",
        "                item_idx = book_mapping[book_id]  # Índice original del ítem\n",
        "                filtered_idx = book_id_to_filtered_idx[book_id]  # Índice filtrado para embeddings\n",
        "                self.samples.append((user_id, item_idx, filtered_idx, label))\n",
        "\n",
        "        self.user_embeddings_gcn = user_embeddings_gcn\n",
        "        self.item_embeddings_gcn = item_embeddings_gcn_filtered\n",
        "        self.tfidf_tensor = tfidf_tensor_filtered\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_id, item_idx, filtered_idx, label = self.samples[idx]\n",
        "        gcn_user = self.user_embeddings_gcn[user_id]\n",
        "        gcn_item = self.item_embeddings_gcn[filtered_idx]\n",
        "        tfidf_item = self.tfidf_tensor[filtered_idx]\n",
        "        gcn_emb = gcn_user * gcn_item  # Producto elemento a elemento entre embeddings\n",
        "        return gcn_emb, tfidf_item, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def split_interactions(interactions, train_frac=0.8, val_frac=0.1, test_frac=0.1, seed=42):\n",
        "    random.seed(seed)\n",
        "    random.shuffle(interactions)  # Barajamos las interacciones\n",
        "    n = len(interactions)\n",
        "    n_train = int(n * train_frac)\n",
        "    n_val = int(n * val_frac)\n",
        "    train = interactions[:n_train]\n",
        "    val = interactions[n_train:n_train + n_val]\n",
        "    test = interactions[n_train + n_val:]  # El resto va a test\n",
        "    return train, val, test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ad81f7d",
      "metadata": {
        "id": "5ad81f7d"
      },
      "source": [
        "### Métricas de evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279b773a",
      "metadata": {
        "id": "279b773a"
      },
      "source": [
        "Ahora, implementamos las funciones necesarias para evaluar el modelo híbrido:\n",
        "\n",
        "- La función `recommend_top_k` genera recomendaciones personalizadas para un usuario dado, combinando embeddings de LightGCN y TF-IDF a través del modelo entrenado.\n",
        "\n",
        "- A partir de estas recomendaciones, se calculan métricas estándar de evaluación en sistemas de recomendación como `precisión@k`, `recall@k` y `NDCG@k`.\n",
        "\n",
        "- Por último, la función `evaluate` aplica estas métricas sobre todos los usuarios del conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e05d3a49",
      "metadata": {
        "id": "e05d3a49"
      },
      "outputs": [],
      "source": [
        "def recommend_top_k(model, user_id, user_embeddings, item_embeddings_gcn_filtered, tfidf_tensor_filtered, valid_book_ids, k=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Expandimos el embedding del usuario para alinearlo con todos los ítems\n",
        "        user_emb = user_embeddings[user_id].unsqueeze(0).repeat(item_embeddings_gcn_filtered.shape[0], 1)\n",
        "        gcn_emb = user_emb * item_embeddings_gcn_filtered  # Interacción tipo GCN (producto elemento a elemento)\n",
        "        tfidf_emb = tfidf_tensor_filtered  # Embeddings del contenido\n",
        "        scores = model(gcn_emb, tfidf_emb)  # Puntuaciones de relevancia\n",
        "        topk_indices = torch.topk(scores, k=k).indices.cpu().numpy()  # Indices de los top-k ítems\n",
        "        return [valid_book_ids[i] for i in topk_indices]  # Devolvemos los book_ids correspondientes\n",
        "\n",
        "# Métrica de precisión@k: proporción de ítems recomendados que están en el conjunto relevante\n",
        "def precision_at_k(preds, ground_truth, k):\n",
        "    hits = len(set(preds[:k]) & set(ground_truth))\n",
        "    return hits / k\n",
        "\n",
        "# Métrica de recall@k: proporción del total de ítems relevantes que fueron recomendados\n",
        "def recall_at_k(preds, ground_truth, k):\n",
        "    hits = len(set(preds[:k]) & set(ground_truth))\n",
        "    return hits / len(ground_truth) if ground_truth else 0\n",
        "\n",
        "# Métrica NDCG@k: mide el orden de los ítems relevantes en el ranking\n",
        "def ndcg_at_k(preds, ground_truth, k):\n",
        "    dcg = sum(1.0 / np.log2(i + 2) for i, item in enumerate(preds[:k]) if item in ground_truth)\n",
        "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(ground_truth), k)))\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evalúa el modelo recorriendo todos los usuarios y agregando las métricas promedio\n",
        "def evaluate(model, user_to_books, user_embeddings, item_embeddings_gcn_filtered, tfidf_tensor_filtered, valid_book_ids, k=20):\n",
        "    precisions, recalls, ndcgs = [], [], []\n",
        "    for user_id, gt_books in user_to_books.items():\n",
        "        if not gt_books:\n",
        "            continue  # Saltamos usuarios sin libros relevantes\n",
        "        recommended = recommend_top_k(model, user_id, user_embeddings, item_embeddings_gcn_filtered, tfidf_tensor_filtered, valid_book_ids, k)\n",
        "        precisions.append(precision_at_k(recommended, gt_books, k))\n",
        "        recalls.append(recall_at_k(recommended, gt_books, k))\n",
        "        ndcgs.append(ndcg_at_k(recommended, gt_books, k))\n",
        "    return np.mean(precisions), np.mean(recalls), np.mean(ndcgs)  # Métricas promedio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a68a86",
      "metadata": {
        "id": "38a68a86"
      },
      "source": [
        "### Funciones de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57d6708",
      "metadata": {
        "id": "a57d6708"
      },
      "source": [
        "Definimos dos funciones para el entrenamiento del modelo híbrido:\n",
        "- `build_user_to_books`: construye un diccionario que mapea cada usuario a los ítems con los que ha interactuado positivamente.\n",
        "- `train_hybrid_model`: entrena la red neuronal híbrida usando embeddings de LightGCN y representaciones TF-IDF.\n",
        "- `hyperparameter_search`: busca la mejor combinación de hiperparámetros para maximizar el valor de nDCG@20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d670dc",
      "metadata": {
        "id": "58d670dc"
      },
      "outputs": [],
      "source": [
        "# Construye un diccionario {usuario: set de libros con los que ha interactuado positivamente}\n",
        "def build_user_to_books(interactions):\n",
        "    user_to_books = {}\n",
        "    for u, i, label in interactions:\n",
        "        if label == 1:  # Solo interacciones positivas\n",
        "            if u not in user_to_books:\n",
        "                user_to_books[u] = set()\n",
        "            user_to_books[u].add(i)\n",
        "    return user_to_books\n",
        "\n",
        "# Entrena el modelo híbrido con validación y early stopping\n",
        "def train_hybrid_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    val_user_to_books,\n",
        "    user_embeddings,\n",
        "    item_embeddings_gcn_filtered,\n",
        "    tfidf_tensor_filtered,\n",
        "    valid_book_ids,\n",
        "    epochs=50,\n",
        "    lr=1e-3,\n",
        "    patience=5,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for gcn_emb, tfidf_emb, label in train_loader:\n",
        "            gcn_emb, tfidf_emb, label = gcn_emb.to(device), tfidf_emb.to(device), label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(gcn_emb, tfidf_emb)\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validación\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for gcn_emb, tfidf_emb, label in val_loader:\n",
        "                gcn_emb, tfidf_emb, label = gcn_emb.to(device), tfidf_emb.to(device), label.to(device)\n",
        "                output = model(gcn_emb, tfidf_emb)\n",
        "                loss = criterion(output, label)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            best_train_losses = train_losses.copy()\n",
        "            best_val_losses = val_losses.copy()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping en epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        precision, recall, ndcg = evaluate(\n",
        "            model,\n",
        "            val_user_to_books,\n",
        "            user_embeddings,\n",
        "            item_embeddings_gcn_filtered,\n",
        "            tfidf_tensor_filtered,\n",
        "            valid_book_ids,\n",
        "            k=20\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | P@20: {precision:.4f} | R@20: {recall:.4f} | nDCG@20: {ndcg:.4f}\")\n",
        "\n",
        "    # Carga el mejor modelo entrenado\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, best_train_losses, best_val_losses\n",
        "\n",
        "# Búsqueda de hiperparámetros con guardado de curvas de pérdida\n",
        "def hyperparameter_search(\n",
        "    train_data,\n",
        "    val_data,\n",
        "    val_user_to_books,\n",
        "    user_embeddings_gcn,\n",
        "    item_embeddings_gcn_filtered,\n",
        "    tfidf_tensor_filtered,\n",
        "    book_mapping,\n",
        "    valid_book_ids,\n",
        "    param_grid,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    grid = list(ParameterGrid(param_grid))\n",
        "    best_ndcg = 0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "    best_train_losses = None\n",
        "    best_val_losses = None\n",
        "\n",
        "    for params in grid:\n",
        "        print(f\"\\nProbando con: {params}\")\n",
        "\n",
        "        model = HybridRecommenderNN(\n",
        "            gcn_dim=user_embeddings_gcn.shape[1],\n",
        "            tfidf_dim=tfidf_tensor_filtered.shape[1],\n",
        "            hidden_dims=params.get('hidden_dims', [128, 64]),\n",
        "            dropout_rate=params.get('dropout_rate', 0.1)\n",
        "        )\n",
        "\n",
        "        train_dataset = HybridDataset(train_data, user_embeddings_gcn, item_embeddings_gcn_filtered, tfidf_tensor_filtered, book_mapping, valid_book_ids)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=params.get('batch_size', 512), shuffle=True)\n",
        "\n",
        "        val_dataset = HybridDataset(val_data, user_embeddings_gcn, item_embeddings_gcn_filtered, tfidf_tensor_filtered, book_mapping, valid_book_ids)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=params.get('batch_size', 512), shuffle=False)\n",
        "\n",
        "        model, train_losses, val_losses = train_hybrid_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            val_user_to_books=val_user_to_books,\n",
        "            user_embeddings=user_embeddings_gcn,\n",
        "            item_embeddings_gcn_filtered=item_embeddings_gcn_filtered,\n",
        "            tfidf_tensor_filtered=tfidf_tensor_filtered,\n",
        "            valid_book_ids=valid_book_ids,\n",
        "            epochs=EPOCHS,\n",
        "            lr=params.get('learning_rate', 1e-3),\n",
        "            patience=PATIENCE,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        precision, recall, ndcg = evaluate(\n",
        "            model,\n",
        "            val_user_to_books,\n",
        "            user_embeddings_gcn,\n",
        "            item_embeddings_gcn_filtered,\n",
        "            tfidf_tensor_filtered,\n",
        "            valid_book_ids,\n",
        "            k=20\n",
        "        )\n",
        "\n",
        "        print(f\"nDCG@20 para esta configuración: {ndcg:.4f}\")\n",
        "\n",
        "        if ndcg > best_ndcg:\n",
        "            best_ndcg = ndcg\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "            best_train_losses = train_losses\n",
        "            best_val_losses = val_losses\n",
        "\n",
        "    print(\"\\n=== Mejor configuración encontrada ===\")\n",
        "    print(best_params)\n",
        "    print(f\"Mejor nDCG@20: {best_ndcg:.4f}\")\n",
        "\n",
        "    return best_model, best_params, best_train_losses, best_val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08be020d",
      "metadata": {
        "id": "08be020d"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ed279b5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed279b5c",
        "outputId": "229cf689-867b-4529-cdc9-64bad9d5ae6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probando con: {'batch_size': 256, 'dropout_rate': 0.1, 'hidden_dims': [128, 64], 'learning_rate': 0.001}\n",
            "Epoch 1 | Train Loss: 0.3386 | Val Loss: 0.2941 | P@20: 0.0423 | R@20: 0.6760 | nDCG@20: 0.3707\n",
            "Epoch 2 | Train Loss: 0.2903 | Val Loss: 0.2807 | P@20: 0.0423 | R@20: 0.6771 | nDCG@20: 0.3795\n",
            "Epoch 3 | Train Loss: 0.2774 | Val Loss: 0.2751 | P@20: 0.0421 | R@20: 0.6764 | nDCG@20: 0.3692\n",
            "Epoch 4 | Train Loss: 0.2688 | Val Loss: 0.2727 | P@20: 0.0419 | R@20: 0.6733 | nDCG@20: 0.3701\n",
            "Epoch 5 | Train Loss: 0.2803 | Val Loss: 0.2657 | P@20: 0.0421 | R@20: 0.6755 | nDCG@20: 0.3615\n",
            "Epoch 6 | Train Loss: 0.2615 | Val Loss: 0.2725 | P@20: 0.0426 | R@20: 0.6819 | nDCG@20: 0.3642\n",
            "Epoch 7 | Train Loss: 0.2559 | Val Loss: 0.2730 | P@20: 0.0418 | R@20: 0.6694 | nDCG@20: 0.3593\n",
            "Epoch 8 | Train Loss: 0.2525 | Val Loss: 0.2703 | P@20: 0.0416 | R@20: 0.6683 | nDCG@20: 0.3520\n",
            "Epoch 9 | Train Loss: 0.2453 | Val Loss: 0.2738 | P@20: 0.0420 | R@20: 0.6738 | nDCG@20: 0.3607\n",
            "Epoch 10 | Train Loss: 0.2458 | Val Loss: 0.2677 | P@20: 0.0421 | R@20: 0.6771 | nDCG@20: 0.3532\n",
            "Epoch 11 | Train Loss: 0.2466 | Val Loss: 0.2669 | P@20: 0.0418 | R@20: 0.6694 | nDCG@20: 0.3517\n",
            "Epoch 12 | Train Loss: 0.2381 | Val Loss: 0.2696 | P@20: 0.0419 | R@20: 0.6740 | nDCG@20: 0.3554\n",
            "Early stopping en epoch 13\n",
            "nDCG@20 para esta configuración: 0.3558\n",
            "\n",
            "Probando con: {'batch_size': 256, 'dropout_rate': 0.1, 'hidden_dims': [128, 64], 'learning_rate': 0.0005}\n",
            "Epoch 1 | Train Loss: 0.3727 | Val Loss: 0.2954 | P@20: 0.0421 | R@20: 0.6728 | nDCG@20: 0.3568\n",
            "Epoch 2 | Train Loss: 0.2947 | Val Loss: 0.2886 | P@20: 0.0426 | R@20: 0.6781 | nDCG@20: 0.3725\n",
            "Epoch 3 | Train Loss: 0.2877 | Val Loss: 0.2826 | P@20: 0.0425 | R@20: 0.6790 | nDCG@20: 0.3735\n",
            "Epoch 4 | Train Loss: 0.2776 | Val Loss: 0.2759 | P@20: 0.0423 | R@20: 0.6755 | nDCG@20: 0.3717\n",
            "Epoch 5 | Train Loss: 0.2851 | Val Loss: 0.2776 | P@20: 0.0425 | R@20: 0.6800 | nDCG@20: 0.3701\n",
            "Epoch 6 | Train Loss: 0.2828 | Val Loss: 0.2787 | P@20: 0.0423 | R@20: 0.6766 | nDCG@20: 0.3664\n",
            "Epoch 7 | Train Loss: 0.2656 | Val Loss: 0.2678 | P@20: 0.0423 | R@20: 0.6773 | nDCG@20: 0.3698\n",
            "Epoch 8 | Train Loss: 0.2606 | Val Loss: 0.2689 | P@20: 0.0422 | R@20: 0.6748 | nDCG@20: 0.3662\n",
            "Epoch 9 | Train Loss: 0.2605 | Val Loss: 0.2695 | P@20: 0.0418 | R@20: 0.6691 | nDCG@20: 0.3588\n",
            "Epoch 10 | Train Loss: 0.2581 | Val Loss: 0.2711 | P@20: 0.0421 | R@20: 0.6717 | nDCG@20: 0.3627\n",
            "Epoch 11 | Train Loss: 0.2521 | Val Loss: 0.2688 | P@20: 0.0426 | R@20: 0.6828 | nDCG@20: 0.3699\n",
            "Epoch 12 | Train Loss: 0.2494 | Val Loss: 0.2675 | P@20: 0.0423 | R@20: 0.6787 | nDCG@20: 0.3621\n",
            "Epoch 13 | Train Loss: 0.2478 | Val Loss: 0.2651 | P@20: 0.0420 | R@20: 0.6740 | nDCG@20: 0.3588\n",
            "Epoch 14 | Train Loss: 0.2475 | Val Loss: 0.2655 | P@20: 0.0414 | R@20: 0.6622 | nDCG@20: 0.3567\n",
            "Epoch 15 | Train Loss: 0.2440 | Val Loss: 0.2671 | P@20: 0.0414 | R@20: 0.6650 | nDCG@20: 0.3488\n",
            "nDCG@20 para esta configuración: 0.3488\n",
            "\n",
            "Probando con: {'batch_size': 256, 'dropout_rate': 0.1, 'hidden_dims': [256, 128], 'learning_rate': 0.001}\n",
            "Epoch 1 | Train Loss: 0.3114 | Val Loss: 0.2801 | P@20: 0.0428 | R@20: 0.6859 | nDCG@20: 0.3850\n",
            "Epoch 2 | Train Loss: 0.2823 | Val Loss: 0.2713 | P@20: 0.0419 | R@20: 0.6703 | nDCG@20: 0.3751\n",
            "Epoch 3 | Train Loss: 0.2747 | Val Loss: 0.2704 | P@20: 0.0425 | R@20: 0.6810 | nDCG@20: 0.3734\n",
            "Epoch 4 | Train Loss: 0.2615 | Val Loss: 0.2717 | P@20: 0.0420 | R@20: 0.6745 | nDCG@20: 0.3704\n",
            "Epoch 5 | Train Loss: 0.2579 | Val Loss: 0.2726 | P@20: 0.0416 | R@20: 0.6700 | nDCG@20: 0.3659\n",
            "Epoch 6 | Train Loss: 0.2514 | Val Loss: 0.2794 | P@20: 0.0416 | R@20: 0.6661 | nDCG@20: 0.3645\n",
            "Epoch 7 | Train Loss: 0.2597 | Val Loss: 0.2696 | P@20: 0.0409 | R@20: 0.6553 | nDCG@20: 0.3411\n",
            "Epoch 8 | Train Loss: 0.2469 | Val Loss: 0.2706 | P@20: 0.0413 | R@20: 0.6613 | nDCG@20: 0.3650\n",
            "Epoch 9 | Train Loss: 0.2364 | Val Loss: 0.2778 | P@20: 0.0416 | R@20: 0.6689 | nDCG@20: 0.3594\n",
            "Epoch 10 | Train Loss: 0.2430 | Val Loss: 0.2747 | P@20: 0.0414 | R@20: 0.6636 | nDCG@20: 0.3588\n",
            "Epoch 11 | Train Loss: 0.2358 | Val Loss: 0.2723 | P@20: 0.0415 | R@20: 0.6664 | nDCG@20: 0.3628\n",
            "Epoch 12 | Train Loss: 0.2270 | Val Loss: 0.2694 | P@20: 0.0418 | R@20: 0.6729 | nDCG@20: 0.3652\n",
            "Epoch 13 | Train Loss: 0.2236 | Val Loss: 0.2715 | P@20: 0.0412 | R@20: 0.6602 | nDCG@20: 0.3561\n",
            "Epoch 14 | Train Loss: 0.2303 | Val Loss: 0.2828 | P@20: 0.0417 | R@20: 0.6704 | nDCG@20: 0.3559\n",
            "Epoch 15 | Train Loss: 0.2241 | Val Loss: 0.2799 | P@20: 0.0397 | R@20: 0.6377 | nDCG@20: 0.3400\n",
            "nDCG@20 para esta configuración: 0.3400\n",
            "\n",
            "Probando con: {'batch_size': 256, 'dropout_rate': 0.1, 'hidden_dims': [256, 128], 'learning_rate': 0.0005}\n",
            "Epoch 1 | Train Loss: 0.3216 | Val Loss: 0.2864 | P@20: 0.0429 | R@20: 0.6877 | nDCG@20: 0.3811\n",
            "Epoch 2 | Train Loss: 0.2912 | Val Loss: 0.2787 | P@20: 0.0427 | R@20: 0.6832 | nDCG@20: 0.3784\n",
            "Epoch 3 | Train Loss: 0.2747 | Val Loss: 0.2731 | P@20: 0.0425 | R@20: 0.6807 | nDCG@20: 0.3730\n",
            "Epoch 4 | Train Loss: 0.2806 | Val Loss: 0.2693 | P@20: 0.0425 | R@20: 0.6801 | nDCG@20: 0.3774\n",
            "Epoch 5 | Train Loss: 0.2644 | Val Loss: 0.2649 | P@20: 0.0423 | R@20: 0.6777 | nDCG@20: 0.3660\n",
            "Epoch 6 | Train Loss: 0.2575 | Val Loss: 0.2641 | P@20: 0.0425 | R@20: 0.6819 | nDCG@20: 0.3673\n",
            "Epoch 7 | Train Loss: 0.2521 | Val Loss: 0.2631 | P@20: 0.0417 | R@20: 0.6683 | nDCG@20: 0.3639\n",
            "Epoch 8 | Train Loss: 0.2624 | Val Loss: 0.2844 | P@20: 0.0414 | R@20: 0.6636 | nDCG@20: 0.3533\n",
            "Epoch 9 | Train Loss: 0.2512 | Val Loss: 0.2617 | P@20: 0.0419 | R@20: 0.6730 | nDCG@20: 0.3598\n",
            "Epoch 10 | Train Loss: 0.2411 | Val Loss: 0.2638 | P@20: 0.0415 | R@20: 0.6667 | nDCG@20: 0.3609\n",
            "Epoch 11 | Train Loss: 0.2361 | Val Loss: 0.2650 | P@20: 0.0417 | R@20: 0.6729 | nDCG@20: 0.3677\n",
            "Epoch 12 | Train Loss: 0.2380 | Val Loss: 0.2689 | P@20: 0.0416 | R@20: 0.6712 | nDCG@20: 0.3623\n",
            "Epoch 13 | Train Loss: 0.2344 | Val Loss: 0.2636 | P@20: 0.0421 | R@20: 0.6758 | nDCG@20: 0.3670\n",
            "Epoch 14 | Train Loss: 0.2389 | Val Loss: 0.2676 | P@20: 0.0411 | R@20: 0.6607 | nDCG@20: 0.3483\n",
            "Epoch 15 | Train Loss: 0.2296 | Val Loss: 0.2598 | P@20: 0.0415 | R@20: 0.6679 | nDCG@20: 0.3593\n",
            "nDCG@20 para esta configuración: 0.3593\n",
            "\n",
            "Probando con: {'batch_size': 256, 'dropout_rate': 0.3, 'hidden_dims': [128, 64], 'learning_rate': 0.001}\n",
            "Epoch 1 | Train Loss: 0.3554 | Val Loss: 0.2894 | P@20: 0.0426 | R@20: 0.6824 | nDCG@20: 0.3759\n",
            "Epoch 2 | Train Loss: 0.3063 | Val Loss: 0.2851 | P@20: 0.0424 | R@20: 0.6776 | nDCG@20: 0.3746\n",
            "Epoch 3 | Train Loss: 0.2936 | Val Loss: 0.2838 | P@20: 0.0426 | R@20: 0.6801 | nDCG@20: 0.3771\n",
            "Epoch 4 | Train Loss: 0.2942 | Val Loss: 0.2826 | P@20: 0.0423 | R@20: 0.6759 | nDCG@20: 0.3658\n",
            "Epoch 5 | Train Loss: 0.2830 | Val Loss: 0.2773 | P@20: 0.0423 | R@20: 0.6787 | nDCG@20: 0.3715\n",
            "Epoch 6 | Train Loss: 0.2767 | Val Loss: 0.2721 | P@20: 0.0423 | R@20: 0.6791 | nDCG@20: 0.3717\n",
            "Epoch 7 | Train Loss: 0.2788 | Val Loss: 0.2808 | P@20: 0.0421 | R@20: 0.6719 | nDCG@20: 0.3705\n",
            "Epoch 8 | Train Loss: 0.2705 | Val Loss: 0.2704 | P@20: 0.0425 | R@20: 0.6825 | nDCG@20: 0.3742\n",
            "Epoch 9 | Train Loss: 0.2798 | Val Loss: 0.2685 | P@20: 0.0423 | R@20: 0.6784 | nDCG@20: 0.3742\n",
            "Epoch 10 | Train Loss: 0.2722 | Val Loss: 0.2732 | P@20: 0.0419 | R@20: 0.6737 | nDCG@20: 0.3656\n",
            "Epoch 11 | Train Loss: 0.2742 | Val Loss: 0.2706 | P@20: 0.0419 | R@20: 0.6726 | nDCG@20: 0.3717\n",
            "Epoch 12 | Train Loss: 0.2709 | Val Loss: 0.2709 | P@20: 0.0421 | R@20: 0.6773 | nDCG@20: 0.3678\n",
            "Epoch 13 | Train Loss: 0.2738 | Val Loss: 0.2725 | P@20: 0.0424 | R@20: 0.6789 | nDCG@20: 0.3711\n",
            "Epoch 14 | Train Loss: 0.2761 | Val Loss: 0.2792 | P@20: 0.0419 | R@20: 0.6726 | nDCG@20: 0.3630\n",
            "Epoch 15 | Train Loss: 0.2634 | Val Loss: 0.2728 | P@20: 0.0422 | R@20: 0.6743 | nDCG@20: 0.3608\n",
            "nDCG@20 para esta configuración: 0.3608\n",
            "\n",
            "Probando con: {'batch_size': 256, 'dropout_rate': 0.3, 'hidden_dims': [128, 64], 'learning_rate': 0.0005}\n",
            "Epoch 1 | Train Loss: 0.3744 | Val Loss: 0.2959 | P@20: 0.0423 | R@20: 0.6774 | nDCG@20: 0.3741\n",
            "Epoch 2 | Train Loss: 0.3140 | Val Loss: 0.2873 | P@20: 0.0426 | R@20: 0.6822 | nDCG@20: 0.3803\n",
            "Epoch 3 | Train Loss: 0.2982 | Val Loss: 0.2847 | P@20: 0.0426 | R@20: 0.6830 | nDCG@20: 0.3812\n",
            "Epoch 4 | Train Loss: 0.2930 | Val Loss: 0.2821 | P@20: 0.0426 | R@20: 0.6819 | nDCG@20: 0.3834\n",
            "Epoch 5 | Train Loss: 0.2884 | Val Loss: 0.2808 | P@20: 0.0425 | R@20: 0.6794 | nDCG@20: 0.3831\n",
            "Epoch 6 | Train Loss: 0.3010 | Val Loss: 0.2805 | P@20: 0.0426 | R@20: 0.6821 | nDCG@20: 0.3838\n",
            "Epoch 7 | Train Loss: 0.2872 | Val Loss: 0.2760 | P@20: 0.0428 | R@20: 0.6873 | nDCG@20: 0.3844\n",
            "Epoch 8 | Train Loss: 0.2795 | Val Loss: 0.2785 | P@20: 0.0425 | R@20: 0.6794 | nDCG@20: 0.3784\n",
            "Epoch 9 | Train Loss: 0.2865 | Val Loss: 0.2727 | P@20: 0.0426 | R@20: 0.6825 | nDCG@20: 0.3776\n",
            "Epoch 10 | Train Loss: 0.2832 | Val Loss: 0.2741 | P@20: 0.0423 | R@20: 0.6769 | nDCG@20: 0.3762\n",
            "Epoch 11 | Train Loss: 0.2731 | Val Loss: 0.2765 | P@20: 0.0423 | R@20: 0.6783 | nDCG@20: 0.3752\n",
            "Epoch 12 | Train Loss: 0.2718 | Val Loss: 0.2786 | P@20: 0.0425 | R@20: 0.6803 | nDCG@20: 0.3797\n",
            "Epoch 13 | Train Loss: 0.2795 | Val Loss: 0.2797 | P@20: 0.0426 | R@20: 0.6848 | nDCG@20: 0.3768\n",
            "Epoch 14 | Train Loss: 0.2839 | Val Loss: 0.2705 | P@20: 0.0422 | R@20: 0.6770 | nDCG@20: 0.3727\n",
            "Epoch 15 | Train Loss: 0.2689 | Val Loss: 0.2711 | P@20: 0.0424 | R@20: 0.6785 | nDCG@20: 0.3743\n",
            "nDCG@20 para esta configuración: 0.3743\n",
            "\n",
            "Probando con: {'batch_size': 256, 'dropout_rate': 0.3, 'hidden_dims': [256, 128], 'learning_rate': 0.001}\n",
            "Epoch 1 | Train Loss: 0.3237 | Val Loss: 0.2865 | P@20: 0.0427 | R@20: 0.6824 | nDCG@20: 0.3800\n",
            "Epoch 2 | Train Loss: 0.2876 | Val Loss: 0.2822 | P@20: 0.0424 | R@20: 0.6790 | nDCG@20: 0.3784\n",
            "Epoch 3 | Train Loss: 0.2902 | Val Loss: 0.2914 | P@20: 0.0423 | R@20: 0.6761 | nDCG@20: 0.3716\n",
            "Epoch 4 | Train Loss: 0.2896 | Val Loss: 0.2763 | P@20: 0.0424 | R@20: 0.6789 | nDCG@20: 0.3786\n",
            "Epoch 5 | Train Loss: 0.2706 | Val Loss: 0.2711 | P@20: 0.0423 | R@20: 0.6766 | nDCG@20: 0.3737\n",
            "Epoch 6 | Train Loss: 0.2691 | Val Loss: 0.2675 | P@20: 0.0424 | R@20: 0.6785 | nDCG@20: 0.3747\n",
            "Epoch 7 | Train Loss: 0.2689 | Val Loss: 0.2689 | P@20: 0.0424 | R@20: 0.6777 | nDCG@20: 0.3733\n",
            "Epoch 8 | Train Loss: 0.2768 | Val Loss: 0.2666 | P@20: 0.0425 | R@20: 0.6796 | nDCG@20: 0.3711\n",
            "Epoch 9 | Train Loss: 0.2586 | Val Loss: 0.2667 | P@20: 0.0424 | R@20: 0.6781 | nDCG@20: 0.3678\n",
            "Epoch 10 | Train Loss: 0.2640 | Val Loss: 0.2757 | P@20: 0.0414 | R@20: 0.6647 | nDCG@20: 0.3535\n",
            "Epoch 11 | Train Loss: 0.2557 | Val Loss: 0.2731 | P@20: 0.0425 | R@20: 0.6822 | nDCG@20: 0.3698\n",
            "Epoch 12 | Train Loss: 0.2598 | Val Loss: 0.2871 | P@20: 0.0421 | R@20: 0.6765 | nDCG@20: 0.3673\n",
            "Epoch 13 | Train Loss: 0.2623 | Val Loss: 0.2708 | P@20: 0.0423 | R@20: 0.6787 | nDCG@20: 0.3696\n",
            "Epoch 14 | Train Loss: 0.2666 | Val Loss: 0.2697 | P@20: 0.0418 | R@20: 0.6694 | nDCG@20: 0.3643\n",
            "Epoch 15 | Train Loss: 0.2503 | Val Loss: 0.2692 | P@20: 0.0421 | R@20: 0.6755 | nDCG@20: 0.3642\n",
            "nDCG@20 para esta configuración: 0.3642\n",
            "\n",
            "Probando con: {'batch_size': 256, 'dropout_rate': 0.3, 'hidden_dims': [256, 128], 'learning_rate': 0.0005}\n",
            "Epoch 1 | Train Loss: 0.3436 | Val Loss: 0.2906 | P@20: 0.0424 | R@20: 0.6807 | nDCG@20: 0.3712\n",
            "Epoch 2 | Train Loss: 0.3033 | Val Loss: 0.2834 | P@20: 0.0427 | R@20: 0.6841 | nDCG@20: 0.3790\n",
            "Epoch 3 | Train Loss: 0.2896 | Val Loss: 0.2778 | P@20: 0.0422 | R@20: 0.6766 | nDCG@20: 0.3750\n",
            "Epoch 4 | Train Loss: 0.2859 | Val Loss: 0.2836 | P@20: 0.0424 | R@20: 0.6778 | nDCG@20: 0.3809\n",
            "Epoch 5 | Train Loss: 0.2830 | Val Loss: 0.2709 | P@20: 0.0422 | R@20: 0.6753 | nDCG@20: 0.3814\n",
            "Epoch 6 | Train Loss: 0.2743 | Val Loss: 0.2705 | P@20: 0.0422 | R@20: 0.6744 | nDCG@20: 0.3863\n",
            "Epoch 7 | Train Loss: 0.2679 | Val Loss: 0.2719 | P@20: 0.0421 | R@20: 0.6736 | nDCG@20: 0.3810\n",
            "Epoch 8 | Train Loss: 0.2724 | Val Loss: 0.2735 | P@20: 0.0422 | R@20: 0.6762 | nDCG@20: 0.3782\n",
            "Epoch 9 | Train Loss: 0.2657 | Val Loss: 0.2705 | P@20: 0.0417 | R@20: 0.6668 | nDCG@20: 0.3780\n",
            "Epoch 10 | Train Loss: 0.2635 | Val Loss: 0.2704 | P@20: 0.0419 | R@20: 0.6733 | nDCG@20: 0.3753\n",
            "Epoch 11 | Train Loss: 0.2664 | Val Loss: 0.2695 | P@20: 0.0418 | R@20: 0.6690 | nDCG@20: 0.3736\n",
            "Epoch 12 | Train Loss: 0.2674 | Val Loss: 0.2667 | P@20: 0.0421 | R@20: 0.6751 | nDCG@20: 0.3720\n",
            "Epoch 13 | Train Loss: 0.2700 | Val Loss: 0.2649 | P@20: 0.0421 | R@20: 0.6749 | nDCG@20: 0.3717\n",
            "Epoch 14 | Train Loss: 0.2554 | Val Loss: 0.2670 | P@20: 0.0421 | R@20: 0.6728 | nDCG@20: 0.3700\n",
            "Epoch 15 | Train Loss: 0.2525 | Val Loss: 0.2641 | P@20: 0.0419 | R@20: 0.6712 | nDCG@20: 0.3684\n",
            "nDCG@20 para esta configuración: 0.3684\n",
            "\n",
            "Probando con: {'batch_size': 512, 'dropout_rate': 0.1, 'hidden_dims': [128, 64], 'learning_rate': 0.001}\n",
            "Epoch 1 | Train Loss: 0.3653 | Val Loss: 0.2920 | P@20: 0.0417 | R@20: 0.6670 | nDCG@20: 0.3566\n",
            "Epoch 2 | Train Loss: 0.2920 | Val Loss: 0.2818 | P@20: 0.0424 | R@20: 0.6794 | nDCG@20: 0.3759\n",
            "Epoch 3 | Train Loss: 0.2806 | Val Loss: 0.2746 | P@20: 0.0426 | R@20: 0.6819 | nDCG@20: 0.3722\n",
            "Epoch 4 | Train Loss: 0.2738 | Val Loss: 0.2690 | P@20: 0.0423 | R@20: 0.6773 | nDCG@20: 0.3656\n",
            "Epoch 5 | Train Loss: 0.2661 | Val Loss: 0.2668 | P@20: 0.0418 | R@20: 0.6690 | nDCG@20: 0.3592\n",
            "Epoch 6 | Train Loss: 0.2612 | Val Loss: 0.2647 | P@20: 0.0418 | R@20: 0.6700 | nDCG@20: 0.3574\n",
            "Epoch 7 | Train Loss: 0.2537 | Val Loss: 0.2656 | P@20: 0.0417 | R@20: 0.6687 | nDCG@20: 0.3594\n",
            "Epoch 8 | Train Loss: 0.2485 | Val Loss: 0.2685 | P@20: 0.0418 | R@20: 0.6698 | nDCG@20: 0.3602\n",
            "Epoch 9 | Train Loss: 0.2462 | Val Loss: 0.2672 | P@20: 0.0423 | R@20: 0.6768 | nDCG@20: 0.3589\n",
            "Epoch 10 | Train Loss: 0.2421 | Val Loss: 0.2751 | P@20: 0.0412 | R@20: 0.6624 | nDCG@20: 0.3511\n",
            "Epoch 11 | Train Loss: 0.2414 | Val Loss: 0.2714 | P@20: 0.0417 | R@20: 0.6718 | nDCG@20: 0.3567\n",
            "Epoch 12 | Train Loss: 0.2354 | Val Loss: 0.2710 | P@20: 0.0414 | R@20: 0.6659 | nDCG@20: 0.3544\n",
            "Epoch 13 | Train Loss: 0.2308 | Val Loss: 0.2756 | P@20: 0.0411 | R@20: 0.6620 | nDCG@20: 0.3443\n",
            "Early stopping en epoch 14\n",
            "nDCG@20 para esta configuración: 0.3500\n",
            "\n",
            "Probando con: {'batch_size': 512, 'dropout_rate': 0.1, 'hidden_dims': [128, 64], 'learning_rate': 0.0005}\n",
            "Epoch 1 | Train Loss: 0.4215 | Val Loss: 0.3317 | P@20: 0.0401 | R@20: 0.6444 | nDCG@20: 0.3407\n",
            "Epoch 2 | Train Loss: 0.3106 | Val Loss: 0.2889 | P@20: 0.0419 | R@20: 0.6713 | nDCG@20: 0.3555\n",
            "Epoch 3 | Train Loss: 0.2919 | Val Loss: 0.2801 | P@20: 0.0424 | R@20: 0.6785 | nDCG@20: 0.3744\n",
            "Epoch 4 | Train Loss: 0.2841 | Val Loss: 0.2763 | P@20: 0.0421 | R@20: 0.6739 | nDCG@20: 0.3723\n",
            "Epoch 5 | Train Loss: 0.2789 | Val Loss: 0.2718 | P@20: 0.0421 | R@20: 0.6755 | nDCG@20: 0.3746\n",
            "Epoch 6 | Train Loss: 0.2719 | Val Loss: 0.2703 | P@20: 0.0422 | R@20: 0.6764 | nDCG@20: 0.3746\n",
            "Epoch 7 | Train Loss: 0.2688 | Val Loss: 0.2678 | P@20: 0.0423 | R@20: 0.6767 | nDCG@20: 0.3747\n",
            "Epoch 8 | Train Loss: 0.2639 | Val Loss: 0.2652 | P@20: 0.0420 | R@20: 0.6716 | nDCG@20: 0.3752\n",
            "Epoch 9 | Train Loss: 0.2592 | Val Loss: 0.2650 | P@20: 0.0421 | R@20: 0.6744 | nDCG@20: 0.3750\n",
            "Epoch 10 | Train Loss: 0.2558 | Val Loss: 0.2648 | P@20: 0.0423 | R@20: 0.6780 | nDCG@20: 0.3775\n",
            "Epoch 11 | Train Loss: 0.2537 | Val Loss: 0.2674 | P@20: 0.0418 | R@20: 0.6699 | nDCG@20: 0.3757\n",
            "Epoch 12 | Train Loss: 0.2495 | Val Loss: 0.2659 | P@20: 0.0416 | R@20: 0.6693 | nDCG@20: 0.3701\n",
            "Epoch 13 | Train Loss: 0.2494 | Val Loss: 0.2661 | P@20: 0.0422 | R@20: 0.6756 | nDCG@20: 0.3775\n",
            "Epoch 14 | Train Loss: 0.2430 | Val Loss: 0.2661 | P@20: 0.0422 | R@20: 0.6780 | nDCG@20: 0.3733\n",
            "Epoch 15 | Train Loss: 0.2412 | Val Loss: 0.2703 | P@20: 0.0418 | R@20: 0.6709 | nDCG@20: 0.3732\n",
            "nDCG@20 para esta configuración: 0.3732\n",
            "\n",
            "Probando con: {'batch_size': 512, 'dropout_rate': 0.1, 'hidden_dims': [256, 128], 'learning_rate': 0.001}\n",
            "Epoch 1 | Train Loss: 0.3265 | Val Loss: 0.2855 | P@20: 0.0426 | R@20: 0.6825 | nDCG@20: 0.3829\n",
            "Epoch 2 | Train Loss: 0.2844 | Val Loss: 0.2732 | P@20: 0.0424 | R@20: 0.6755 | nDCG@20: 0.3776\n",
            "Epoch 3 | Train Loss: 0.2744 | Val Loss: 0.2640 | P@20: 0.0428 | R@20: 0.6859 | nDCG@20: 0.3816\n",
            "Epoch 4 | Train Loss: 0.2653 | Val Loss: 0.2626 | P@20: 0.0419 | R@20: 0.6717 | nDCG@20: 0.3721\n",
            "Epoch 5 | Train Loss: 0.2575 | Val Loss: 0.2620 | P@20: 0.0418 | R@20: 0.6698 | nDCG@20: 0.3687\n",
            "Epoch 6 | Train Loss: 0.2517 | Val Loss: 0.2633 | P@20: 0.0410 | R@20: 0.6570 | nDCG@20: 0.3622\n",
            "Epoch 7 | Train Loss: 0.2446 | Val Loss: 0.2626 | P@20: 0.0416 | R@20: 0.6675 | nDCG@20: 0.3615\n",
            "Epoch 8 | Train Loss: 0.2380 | Val Loss: 0.2611 | P@20: 0.0421 | R@20: 0.6744 | nDCG@20: 0.3641\n",
            "Epoch 9 | Train Loss: 0.2334 | Val Loss: 0.2657 | P@20: 0.0411 | R@20: 0.6604 | nDCG@20: 0.3589\n",
            "Epoch 10 | Train Loss: 0.2275 | Val Loss: 0.2766 | P@20: 0.0416 | R@20: 0.6673 | nDCG@20: 0.3579\n",
            "Epoch 11 | Train Loss: 0.2208 | Val Loss: 0.2720 | P@20: 0.0404 | R@20: 0.6483 | nDCG@20: 0.3379\n",
            "Epoch 12 | Train Loss: 0.2201 | Val Loss: 0.2768 | P@20: 0.0407 | R@20: 0.6556 | nDCG@20: 0.3472\n",
            "Epoch 13 | Train Loss: 0.2120 | Val Loss: 0.2785 | P@20: 0.0415 | R@20: 0.6657 | nDCG@20: 0.3551\n",
            "Epoch 14 | Train Loss: 0.2100 | Val Loss: 0.2890 | P@20: 0.0409 | R@20: 0.6583 | nDCG@20: 0.3561\n",
            "Epoch 15 | Train Loss: 0.2052 | Val Loss: 0.2764 | P@20: 0.0391 | R@20: 0.6257 | nDCG@20: 0.3207\n",
            "nDCG@20 para esta configuración: 0.3207\n",
            "\n",
            "Probando con: {'batch_size': 512, 'dropout_rate': 0.1, 'hidden_dims': [256, 128], 'learning_rate': 0.0005}\n",
            "Epoch 1 | Train Loss: 0.3431 | Val Loss: 0.2894 | P@20: 0.0424 | R@20: 0.6790 | nDCG@20: 0.3707\n",
            "Epoch 2 | Train Loss: 0.2884 | Val Loss: 0.2767 | P@20: 0.0427 | R@20: 0.6837 | nDCG@20: 0.3721\n",
            "Epoch 3 | Train Loss: 0.2794 | Val Loss: 0.2707 | P@20: 0.0426 | R@20: 0.6821 | nDCG@20: 0.3743\n",
            "Epoch 4 | Train Loss: 0.2713 | Val Loss: 0.2685 | P@20: 0.0429 | R@20: 0.6885 | nDCG@20: 0.3747\n",
            "Epoch 5 | Train Loss: 0.2638 | Val Loss: 0.2646 | P@20: 0.0428 | R@20: 0.6857 | nDCG@20: 0.3734\n",
            "Epoch 6 | Train Loss: 0.2590 | Val Loss: 0.2637 | P@20: 0.0424 | R@20: 0.6817 | nDCG@20: 0.3708\n",
            "Epoch 7 | Train Loss: 0.2537 | Val Loss: 0.2633 | P@20: 0.0419 | R@20: 0.6731 | nDCG@20: 0.3640\n",
            "Epoch 8 | Train Loss: 0.2482 | Val Loss: 0.2739 | P@20: 0.0426 | R@20: 0.6841 | nDCG@20: 0.3713\n",
            "Epoch 9 | Train Loss: 0.2459 | Val Loss: 0.2639 | P@20: 0.0426 | R@20: 0.6854 | nDCG@20: 0.3676\n",
            "Epoch 10 | Train Loss: 0.2418 | Val Loss: 0.2728 | P@20: 0.0422 | R@20: 0.6785 | nDCG@20: 0.3672\n",
            "Epoch 11 | Train Loss: 0.2355 | Val Loss: 0.2675 | P@20: 0.0426 | R@20: 0.6841 | nDCG@20: 0.3650\n",
            "Epoch 12 | Train Loss: 0.2318 | Val Loss: 0.2636 | P@20: 0.0421 | R@20: 0.6778 | nDCG@20: 0.3646\n",
            "Epoch 13 | Train Loss: 0.2277 | Val Loss: 0.2691 | P@20: 0.0420 | R@20: 0.6758 | nDCG@20: 0.3587\n",
            "Epoch 14 | Train Loss: 0.2237 | Val Loss: 0.2770 | P@20: 0.0422 | R@20: 0.6771 | nDCG@20: 0.3615\n",
            "Early stopping en epoch 15\n",
            "nDCG@20 para esta configuración: 0.3511\n",
            "\n",
            "Probando con: {'batch_size': 512, 'dropout_rate': 0.3, 'hidden_dims': [128, 64], 'learning_rate': 0.001}\n",
            "Epoch 1 | Train Loss: 0.3813 | Val Loss: 0.2987 | P@20: 0.0417 | R@20: 0.6686 | nDCG@20: 0.3636\n",
            "Epoch 2 | Train Loss: 0.3014 | Val Loss: 0.2842 | P@20: 0.0425 | R@20: 0.6814 | nDCG@20: 0.3810\n",
            "Epoch 3 | Train Loss: 0.2953 | Val Loss: 0.2813 | P@20: 0.0429 | R@20: 0.6847 | nDCG@20: 0.3807\n",
            "Epoch 4 | Train Loss: 0.2903 | Val Loss: 0.2762 | P@20: 0.0428 | R@20: 0.6837 | nDCG@20: 0.3811\n",
            "Epoch 5 | Train Loss: 0.2866 | Val Loss: 0.2755 | P@20: 0.0425 | R@20: 0.6794 | nDCG@20: 0.3767\n",
            "Epoch 6 | Train Loss: 0.2814 | Val Loss: 0.2729 | P@20: 0.0427 | R@20: 0.6825 | nDCG@20: 0.3748\n",
            "Epoch 7 | Train Loss: 0.2765 | Val Loss: 0.2698 | P@20: 0.0423 | R@20: 0.6760 | nDCG@20: 0.3698\n",
            "Epoch 8 | Train Loss: 0.2727 | Val Loss: 0.2689 | P@20: 0.0424 | R@20: 0.6763 | nDCG@20: 0.3730\n",
            "Epoch 9 | Train Loss: 0.2704 | Val Loss: 0.2671 | P@20: 0.0423 | R@20: 0.6774 | nDCG@20: 0.3721\n",
            "Epoch 10 | Train Loss: 0.2665 | Val Loss: 0.2685 | P@20: 0.0423 | R@20: 0.6791 | nDCG@20: 0.3674\n",
            "Epoch 11 | Train Loss: 0.2649 | Val Loss: 0.2661 | P@20: 0.0422 | R@20: 0.6769 | nDCG@20: 0.3692\n",
            "Epoch 12 | Train Loss: 0.2639 | Val Loss: 0.2653 | P@20: 0.0422 | R@20: 0.6768 | nDCG@20: 0.3698\n",
            "Epoch 13 | Train Loss: 0.2618 | Val Loss: 0.2668 | P@20: 0.0422 | R@20: 0.6773 | nDCG@20: 0.3705\n",
            "Epoch 14 | Train Loss: 0.2601 | Val Loss: 0.2653 | P@20: 0.0418 | R@20: 0.6716 | nDCG@20: 0.3676\n",
            "Epoch 15 | Train Loss: 0.2556 | Val Loss: 0.2664 | P@20: 0.0419 | R@20: 0.6721 | nDCG@20: 0.3654\n",
            "nDCG@20 para esta configuración: 0.3654\n",
            "\n",
            "Probando con: {'batch_size': 512, 'dropout_rate': 0.3, 'hidden_dims': [128, 64], 'learning_rate': 0.0005}\n",
            "Epoch 1 | Train Loss: 0.4309 | Val Loss: 0.3321 | P@20: 0.0402 | R@20: 0.6455 | nDCG@20: 0.3619\n",
            "Epoch 2 | Train Loss: 0.3210 | Val Loss: 0.2947 | P@20: 0.0417 | R@20: 0.6669 | nDCG@20: 0.3729\n",
            "Epoch 3 | Train Loss: 0.3035 | Val Loss: 0.2864 | P@20: 0.0420 | R@20: 0.6725 | nDCG@20: 0.3761\n",
            "Epoch 4 | Train Loss: 0.2976 | Val Loss: 0.2832 | P@20: 0.0421 | R@20: 0.6742 | nDCG@20: 0.3773\n",
            "Epoch 5 | Train Loss: 0.2947 | Val Loss: 0.2807 | P@20: 0.0423 | R@20: 0.6770 | nDCG@20: 0.3802\n",
            "Epoch 6 | Train Loss: 0.2894 | Val Loss: 0.2784 | P@20: 0.0423 | R@20: 0.6771 | nDCG@20: 0.3781\n",
            "Epoch 7 | Train Loss: 0.2865 | Val Loss: 0.2781 | P@20: 0.0424 | R@20: 0.6787 | nDCG@20: 0.3820\n",
            "Epoch 8 | Train Loss: 0.2827 | Val Loss: 0.2749 | P@20: 0.0425 | R@20: 0.6792 | nDCG@20: 0.3773\n",
            "Epoch 9 | Train Loss: 0.2808 | Val Loss: 0.2736 | P@20: 0.0426 | R@20: 0.6808 | nDCG@20: 0.3755\n",
            "Epoch 10 | Train Loss: 0.2792 | Val Loss: 0.2724 | P@20: 0.0425 | R@20: 0.6786 | nDCG@20: 0.3731\n",
            "Epoch 11 | Train Loss: 0.2764 | Val Loss: 0.2712 | P@20: 0.0425 | R@20: 0.6783 | nDCG@20: 0.3725\n",
            "Epoch 12 | Train Loss: 0.2750 | Val Loss: 0.2691 | P@20: 0.0425 | R@20: 0.6803 | nDCG@20: 0.3745\n",
            "Epoch 13 | Train Loss: 0.2730 | Val Loss: 0.2689 | P@20: 0.0424 | R@20: 0.6791 | nDCG@20: 0.3727\n",
            "Epoch 14 | Train Loss: 0.2706 | Val Loss: 0.2676 | P@20: 0.0425 | R@20: 0.6801 | nDCG@20: 0.3745\n",
            "Epoch 15 | Train Loss: 0.2678 | Val Loss: 0.2688 | P@20: 0.0426 | R@20: 0.6817 | nDCG@20: 0.3750\n",
            "nDCG@20 para esta configuración: 0.3750\n",
            "\n",
            "Probando con: {'batch_size': 512, 'dropout_rate': 0.3, 'hidden_dims': [256, 128], 'learning_rate': 0.001}\n",
            "Epoch 1 | Train Loss: 0.3351 | Val Loss: 0.2855 | P@20: 0.0426 | R@20: 0.6818 | nDCG@20: 0.3765\n",
            "Epoch 2 | Train Loss: 0.2916 | Val Loss: 0.2772 | P@20: 0.0424 | R@20: 0.6747 | nDCG@20: 0.3747\n",
            "Epoch 3 | Train Loss: 0.2834 | Val Loss: 0.2710 | P@20: 0.0423 | R@20: 0.6765 | nDCG@20: 0.3733\n",
            "Epoch 4 | Train Loss: 0.2759 | Val Loss: 0.2686 | P@20: 0.0422 | R@20: 0.6759 | nDCG@20: 0.3777\n",
            "Epoch 5 | Train Loss: 0.2725 | Val Loss: 0.2677 | P@20: 0.0424 | R@20: 0.6781 | nDCG@20: 0.3803\n",
            "Epoch 6 | Train Loss: 0.2670 | Val Loss: 0.2667 | P@20: 0.0425 | R@20: 0.6818 | nDCG@20: 0.3730\n",
            "Epoch 7 | Train Loss: 0.2622 | Val Loss: 0.2668 | P@20: 0.0419 | R@20: 0.6715 | nDCG@20: 0.3661\n",
            "Epoch 8 | Train Loss: 0.2603 | Val Loss: 0.2625 | P@20: 0.0421 | R@20: 0.6772 | nDCG@20: 0.3679\n",
            "Epoch 9 | Train Loss: 0.2581 | Val Loss: 0.2656 | P@20: 0.0419 | R@20: 0.6754 | nDCG@20: 0.3690\n",
            "Epoch 10 | Train Loss: 0.2548 | Val Loss: 0.2684 | P@20: 0.0419 | R@20: 0.6756 | nDCG@20: 0.3706\n",
            "Epoch 11 | Train Loss: 0.2491 | Val Loss: 0.2642 | P@20: 0.0421 | R@20: 0.6772 | nDCG@20: 0.3686\n",
            "Epoch 12 | Train Loss: 0.2474 | Val Loss: 0.2694 | P@20: 0.0421 | R@20: 0.6756 | nDCG@20: 0.3681\n",
            "Epoch 13 | Train Loss: 0.2444 | Val Loss: 0.2668 | P@20: 0.0418 | R@20: 0.6730 | nDCG@20: 0.3634\n",
            "Epoch 14 | Train Loss: 0.2448 | Val Loss: 0.2654 | P@20: 0.0421 | R@20: 0.6762 | nDCG@20: 0.3684\n",
            "Epoch 15 | Train Loss: 0.2433 | Val Loss: 0.2680 | P@20: 0.0421 | R@20: 0.6782 | nDCG@20: 0.3624\n",
            "nDCG@20 para esta configuración: 0.3624\n",
            "\n",
            "Probando con: {'batch_size': 512, 'dropout_rate': 0.3, 'hidden_dims': [256, 128], 'learning_rate': 0.0005}\n",
            "Epoch 1 | Train Loss: 0.3595 | Val Loss: 0.2940 | P@20: 0.0418 | R@20: 0.6700 | nDCG@20: 0.3738\n",
            "Epoch 2 | Train Loss: 0.2967 | Val Loss: 0.2806 | P@20: 0.0426 | R@20: 0.6818 | nDCG@20: 0.3859\n",
            "Epoch 3 | Train Loss: 0.2907 | Val Loss: 0.2763 | P@20: 0.0425 | R@20: 0.6801 | nDCG@20: 0.3808\n",
            "Epoch 4 | Train Loss: 0.2859 | Val Loss: 0.2735 | P@20: 0.0424 | R@20: 0.6781 | nDCG@20: 0.3774\n",
            "Epoch 5 | Train Loss: 0.2798 | Val Loss: 0.2699 | P@20: 0.0423 | R@20: 0.6767 | nDCG@20: 0.3847\n",
            "Epoch 6 | Train Loss: 0.2759 | Val Loss: 0.2687 | P@20: 0.0423 | R@20: 0.6777 | nDCG@20: 0.3831\n",
            "Epoch 7 | Train Loss: 0.2731 | Val Loss: 0.2668 | P@20: 0.0424 | R@20: 0.6795 | nDCG@20: 0.3808\n",
            "Epoch 8 | Train Loss: 0.2688 | Val Loss: 0.2645 | P@20: 0.0424 | R@20: 0.6779 | nDCG@20: 0.3787\n",
            "Epoch 9 | Train Loss: 0.2682 | Val Loss: 0.2646 | P@20: 0.0424 | R@20: 0.6795 | nDCG@20: 0.3808\n",
            "Epoch 10 | Train Loss: 0.2631 | Val Loss: 0.2630 | P@20: 0.0423 | R@20: 0.6774 | nDCG@20: 0.3714\n",
            "Epoch 11 | Train Loss: 0.2621 | Val Loss: 0.2630 | P@20: 0.0421 | R@20: 0.6756 | nDCG@20: 0.3697\n",
            "Epoch 12 | Train Loss: 0.2589 | Val Loss: 0.2633 | P@20: 0.0423 | R@20: 0.6796 | nDCG@20: 0.3727\n",
            "Epoch 13 | Train Loss: 0.2573 | Val Loss: 0.2632 | P@20: 0.0421 | R@20: 0.6760 | nDCG@20: 0.3732\n",
            "Epoch 14 | Train Loss: 0.2545 | Val Loss: 0.2638 | P@20: 0.0421 | R@20: 0.6731 | nDCG@20: 0.3686\n",
            "Epoch 15 | Train Loss: 0.2520 | Val Loss: 0.2648 | P@20: 0.0423 | R@20: 0.6782 | nDCG@20: 0.3693\n",
            "nDCG@20 para esta configuración: 0.3693\n",
            "\n",
            "=== Mejor configuración encontrada ===\n",
            "{'batch_size': 512, 'dropout_rate': 0.3, 'hidden_dims': [128, 64], 'learning_rate': 0.0005}\n",
            "Mejor nDCG@20: 0.3750\n",
            "\n",
            "Mejor configuración encontrada: {'batch_size': 512, 'dropout_rate': 0.3, 'hidden_dims': [128, 64], 'learning_rate': 0.0005}\n"
          ]
        }
      ],
      "source": [
        "# Dividimos los datos en train, val y test\n",
        "train_data, val_data, test_data = split_interactions(interactions_list)\n",
        "\n",
        "# Mapeamos usuario → libros (positivos) para evaluación\n",
        "val_user_to_books = build_user_to_books(val_data)\n",
        "test_user_to_books = build_user_to_books(test_data)\n",
        "\n",
        "# Definimos la grilla de hiperparámetros a explorar\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-3, 5e-4],\n",
        "    'batch_size': [256, 512],\n",
        "    'hidden_dims': [[128, 64], [256, 128]],\n",
        "    'dropout_rate': [0.1, 0.3]\n",
        "}\n",
        "EPOCHS = 15\n",
        "PATIENCE = 8\n",
        "\n",
        "# Búsqueda de hiperparámetros para obtener el mejor conjunto de parámetros\n",
        "best_model, best_params, train_losses, val_losses = hyperparameter_search(\n",
        "    train_data,\n",
        "    val_data,\n",
        "    val_user_to_books,\n",
        "    user_embeddings_gcn,\n",
        "    item_embeddings_gcn_filtered,\n",
        "    tfidf_tensor_filtered,\n",
        "    book_mapping,\n",
        "    valid_book_ids,\n",
        "    param_grid=param_grid,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "print(\"\\nMejor configuración encontrada:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f0f1355e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0f1355e",
        "outputId": "c68c4106-ba49-445d-e323-31e4cd123529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Test results - Recall@20: 0.63768, Precision@20: 0.03941, nDCG@20: 0.34475]\n"
          ]
        }
      ],
      "source": [
        "# Evaluación en test con el modelo entrenado\n",
        "K = 20\n",
        "test_precision, test_recall, test_ndcg = evaluate(\n",
        "    best_model,\n",
        "    test_user_to_books,\n",
        "    user_embeddings_gcn,\n",
        "    item_embeddings_gcn_filtered,\n",
        "    tfidf_tensor_filtered,\n",
        "    valid_book_ids,\n",
        "    k=K\n",
        ")\n",
        "\n",
        "print(f\"[Test results - Recall@{K}: {test_recall:.5f}, Precision@{K}: {test_precision:.5f}, nDCG@{K}: {test_ndcg:.5f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JsH30nd-rISR",
      "metadata": {
        "id": "JsH30nd-rISR"
      },
      "source": [
        "Generamos una gráfica que muestra la evolución de la pérdida durante el entrenamiento y la validación a lo largo de las iteraciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "M7Q6F3grrJMl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "M7Q6F3grrJMl",
        "outputId": "aaa03000-5a58-4ae5-812e-26390c9c985e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generamos una gráfica que muestra la evolución de la pérdida durante el entrenamiento y la validación a lo largo de las iteraciones.\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Entrenamiento')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validación')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.title('Curvas de pérdida de entrenamiento y validación')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X7Gku5AHCL75",
      "metadata": {
        "id": "X7Gku5AHCL75"
      },
      "outputs": [],
      "source": [
        "# Descargar modelo\n",
        "\n",
        "torch.save(best_model.state_dict(), 'HC_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8e6ed8",
      "metadata": {
        "id": "df8e6ed8"
      },
      "source": [
        "## Ejemplo de recomendaciones para usuarios"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076951a7",
      "metadata": {
        "id": "076951a7"
      },
      "source": [
        "Definimos la función `recommend_books_for_user`, que genera una lista de libros recomendados para un usuario, combinando embeddings estructurales (LightGCN) y de contenido (TF-IDF).\n",
        "\n",
        "Por otro lado, `show_user_profile_from_interactions` permite visualizar los libros con los que un usuario ha interactuado positivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "YxR7pNWOzZWN",
      "metadata": {
        "id": "YxR7pNWOzZWN"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()  # Desactiva el cálculo de gradientes durante la inferencia\n",
        "def recommend_books_for_user(user_id, model, user_embeddings_gcn,\n",
        "                             item_embeddings_gcn_filtered, tfidf_tensor_filtered,\n",
        "                             valid_book_ids, book_id_to_title,\n",
        "                             user_seen_books=None, top_k=10,\n",
        "                             device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Validación del ID de usuario\n",
        "    if user_id >= user_embeddings_gcn.shape[0]:\n",
        "        raise ValueError(f\"user_id {user_id} fuera de rango. Máximo permitido: {user_embeddings_gcn.shape[0]-1}\")\n",
        "\n",
        "    # Filtramos los embeddings de usuario y libros\n",
        "    user_embedding = user_embeddings_gcn[user_id].to(device)\n",
        "\n",
        "    scores = []\n",
        "    candidates = []\n",
        "\n",
        "    for idx, book_id in enumerate(valid_book_ids):\n",
        "        # Omitimos libros que el usuario ya ha visto, si se proporciona esa información\n",
        "        if user_seen_books and book_id in user_seen_books:\n",
        "            continue\n",
        "\n",
        "        gcn_item = item_embeddings_gcn_filtered[idx].to(device)\n",
        "        tfidf_item = tfidf_tensor_filtered[idx].to(device)\n",
        "\n",
        "        # Combinación del embedding del usuario con el del ítem (producto elemento a elemento)\n",
        "        gcn_emb = user_embedding * gcn_item\n",
        "\n",
        "        # Inferencia\n",
        "        score = torch.sigmoid(model(gcn_emb.unsqueeze(0), tfidf_item.unsqueeze(0))).item()\n",
        "\n",
        "        scores.append(score)\n",
        "        candidates.append(book_id)\n",
        "\n",
        "    # Selección de los k libros con mayor puntuación\n",
        "    top_k_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "    top_k_books = [candidates[i] for i in top_k_indices]\n",
        "\n",
        "    # Devolvemos una lista con ID de libro, título y puntuación\n",
        "    return [(book_id, book_id_to_title.get(book_id, \"Unknown\"), scores[i]) for i, book_id in zip(top_k_indices, top_k_books)]\n",
        "\n",
        "\n",
        "def show_user_profile_from_interactions(user_id, interactions_list, book_id_to_title):\n",
        "    # Diccionario de usuario -> lista de libros con interacciones positivas\n",
        "    user_interactions = defaultdict(list)\n",
        "    for u_id, b_id, label in interactions_list:\n",
        "        if label == 1:\n",
        "            user_interactions[u_id].append(b_id)\n",
        "\n",
        "    # Extraemos los libros del usuario deseado\n",
        "    user_books = user_interactions.get(user_id, [])\n",
        "\n",
        "    if not user_books:\n",
        "        print(f\"El usuario {user_id} no tiene libros en su perfil.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Perfil de usuario {user_id} - libros leídos/interactuados positivamente:\")\n",
        "    for book_id in user_books:\n",
        "        title = book_id_to_title.get(book_id, \"Título desconocido\")\n",
        "        print(f\"- {title} (Book ID: {book_id})\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a37f5bec",
      "metadata": {
        "id": "a37f5bec"
      },
      "source": [
        "Probamos a recomendar libros a un usuario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7UtwzcCYzaYG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UtwzcCYzaYG",
        "outputId": "a0e271a7-f98c-4cf7-c287-51bf59fb7cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perfil de usuario 14 - libros leídos/interactuados positivamente:\n",
            "- Dicey's Song (Tillerman Cycle, #2) (Book ID: 11831)\n",
            "- Vampire Mountain (Cirque Du Freak, #4) (Book ID: 8960)\n",
            "- Absolutely Mahvelous (Book ID: 10943)\n",
            "- King Dork (King Dork, #1) (Book ID: 10570)\n",
            "- Trials of Death (Cirque Du Freak, #5) (Book ID: 8967)\n",
            "\n",
            "Recomendaciones para el usuario 14:\n",
            " 1. King Dork (King Dork, #1) (Book ID: 10570)\n",
            " 2. The Lake of Souls (Book ID: 8951)\n",
            " 3. Creepy Condors of California (American Chillers, #14) (Book ID: 79640)\n",
            " 4. Cuba 15 (Book ID: 10431)\n",
            " 5. Rebel Angels (Gemma Doyle, #2) (Book ID: 51428)\n",
            " 6. On Fortune's Wheel (Tales of the Kingdom, #2) (Book ID: 95914)\n",
            " 7. Vampire Mountain (Cirque Du Freak, #4) (Book ID: 8960)\n",
            " 8. The Red Dice (The Last Vampire #3) (Book ID: 137970)\n",
            " 9. Trials of Death (Cirque Du Freak, #5) (Book ID: 8967)\n",
            "10. Then He Ate My Boy Entrancers (Confessions of Georgia Nicolson, #6) (Book ID: 112693)\n"
          ]
        }
      ],
      "source": [
        "# Diccionario para mapear book_id a título\n",
        "book_id_to_title = dict(zip(books['book_id'], books['title']))\n",
        "\n",
        "# Ejemplo de usuario\n",
        "user_id = 14\n",
        "\n",
        "# Mostrar perfil del usuario\n",
        "show_user_profile_from_interactions(user_id, interactions_list, book_id_to_title)\n",
        "\n",
        "# Mostrar recomendaciones\n",
        "recommendations = recommend_books_for_user(\n",
        "    user_id=user_id,\n",
        "    model=best_model,\n",
        "    user_embeddings_gcn=user_embeddings_gcn,\n",
        "    item_embeddings_gcn_filtered=item_embeddings_gcn_filtered,\n",
        "    tfidf_tensor_filtered=tfidf_tensor_filtered,\n",
        "    valid_book_ids=valid_book_ids,\n",
        "    book_id_to_title=book_id_to_title,\n",
        "    top_k=10\n",
        ")\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"Recomendaciones para el usuario {user_id}:\")\n",
        "for rank, (book_id, title, score) in enumerate(recommendations, 1):\n",
        "    print(f\"{rank:2d}. {title} (Book ID: {book_id})\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
